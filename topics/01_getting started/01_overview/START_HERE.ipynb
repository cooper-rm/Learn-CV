{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01b4bcbb",
   "metadata": {},
   "source": "# **Demystifying Computer Vision**\n\nComputer vision is a subfield of artificial intelligence (AI) that equips machines with the ability to process, analyze, and interpret visual inputs such as images and videos. It uses machine learning to help computers and other systems derive meaningful information from visual data ([IBM, 2024](https://www.ibm.com/think/topics/computer-vision)).\n\nAt its core, computer vision asks a deceptively hard question: \n\n> *Given raw pixels, how can a computer understand what is happening in the world?*\n\nImages contain pixels, and pixels are just numbers representing colors (*Red, Green, Blue*). Computer vision is about transforming these numbers into meaning.  \n\nThis transformation sits at the intersection of:\n\n- **Linear algebra & geometry** — how images form, how 3D projects to 2D\n- **Probability & optimization** — noise, uncertainty, estimation\n- **Machine learning & deep learning** — learning visual representations\n- **Signal processing** — filters, edges, frequency content\n\nModern computer vision is largely data-driven, but the field remains grounded in classical ideas about geometry, optics, and perception."
  },
  {
   "cell_type": "markdown",
   "id": "hj9zy4bhacs",
   "source": "## Getting Started Path\n\nBefore diving into CV topics, complete these foundational modules:\n\n### [02 — Environment Setup](../02_environment_setup/)\nSet up Python, PyTorch, OpenCV, and your development environment.\n\n### [03 — Python & NumPy Basics](../03_python_numpy_basics/)\nReview essential Python and array manipulation skills.\n\n### [04 — OpenCV Fundamentals](../04_opencv_fundamentals/)\nLearn image loading, manipulation, and basic processing.\n\n### [05 — Git Fundamentals](../05_git_fundamentals/)\nLearn Git workflow and GitHub collaboration for research.\n\n---\n\n## Prerequisites\n\nTo get the most out of this material, you should have:\n\n**Programming**\n- Proficiency in Python (functions, classes, data structures)\n- Familiarity with NumPy for array manipulation\n\n**Mathematics**\n- Linear algebra basics (vectors, matrices, transformations)\n- Calculus fundamentals (derivatives, gradients)\n- Basic probability and statistics\n\n**Machine Learning**\n- Understanding of supervised learning concepts\n- Experience training simple models (optional but helpful)\n\n> Don't worry if you're not an expert in all of these areas. The Getting Started modules will help you build these skills, and we'll review key concepts as they arise.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "cffa6fcc",
   "metadata": {},
   "source": "## What Makes Computer Vision So Difficult?\n\nBeing human means we can solve difficult vision problems with minimal effort.\n\n> Machines are not so lucky.\n\nA cat rotated 30 degrees, partially occluded, under poor lighting, is still obviously a cat to humans. For a machine, that's a distribution shift, a geometry problem, and a generalization test rolled into one.\n\n**Core challenges include:**\n\n- **High dimensionality** — millions of pixels per image\n- **Ambiguity** — 2D images collapse 3D reality\n- **Variability** — lighting, viewpoint, scale, occlusion\n- **Bias** — what you don't see matters\n\nVision is hard because the world is messy. \n\n> That's also why it's valuable."
  },
  {
   "cell_type": "markdown",
   "id": "37b5e473",
   "metadata": {},
   "source": "## Why Should You Care About Computer Vision?\n\nComputer vision is how data science escapes the spreadsheet and touches the physical world.\n\nA huge amount of real-world data is visual: images, videos, medical scans, satellite imagery, sensor feeds, and more.\n\n> *If you can't work with pixels, you're blind to some of the most valuable signals available.*\n\n---\n\n### Practical Applications\n\n- Powers autonomous systems, robotics, and embodied AI \n- Underlies modern healthcare imaging and diagnostics\n- Drives surveillance, mapping, and geospatial intelligence\n- Enables generative models that create and edit visual content\n- Sits at the core of many high-impact ML research problems\n\n---\n\n### Technical Value\n\nComputer vision forces you to confront problems that don't show up in tabular data:\n\n- **High-dimensional inputs** — images have millions of features\n- **Spatial structure and geometry** — position and arrangement matter\n- **Representation learning at scale** — learning what to look for\n- **Robustness under real-world noise** — handling the unexpected\n\n> If you can reason about vision computationally, you tend to reason better about machine learning in general. Vision models stress-test your understanding of optimization, inductive bias, data leakage, and failure modes.\n\n**In short:** Computer vision is where theory meets reality. If you want to work on systems that interact with the world instead of just describing it, this is the place to start."
  },
  {
   "cell_type": "markdown",
   "id": "1fe60fed",
   "metadata": {},
   "source": "## Applications and Research Directions\n\nComputer vision is not a single problem or technique. It's a collection of closely related problem areas, each with its own methods, datasets, and research questions. This repository covers a range of core vision topics:\n\n- Classification & Localization\n- Object Detection\n- Segmentation\n- Pose Estimation\n- Generative Models\n- Visual SLAM\n- And more...\n\n> Each of these areas is an active research space with open problems that matter in both academia and industry.\n\nWe encourage research projects across all of these directions. Some gravitate toward theory and representation learning, others toward geometry, systems, or applied work.\n\n> **All paths are valid here.**\n\nAs you work through the material, read broadly and pay attention to what holds your interest. The goal is not to specialize immediately, but to develop enough exposure to recognize which problems you want to spend time thinking about."
  },
  {
   "cell_type": "markdown",
   "id": "z01zws8m9tk",
   "source": "## How to Use This Repository\n\nThis repository is designed to onboard new MSDS students into computer vision research quickly.\n\n**Recommended path:**\n\n1. **Complete Getting Started** (1-2 days)\n   - [Environment Setup](../02_environment_setup/) — Get your tools ready\n   - [Python & NumPy](../03_python_numpy_basics/) — Review array skills\n   - [OpenCV Fundamentals](../04_opencv_fundamentals/) — Image basics\n   - [Git Fundamentals](../05_git_fundamentals/) — Collaboration skills\n\n2. **Work through foundational topics** — Start with Classification, then follow prerequisites\n3. **Go deep where curious** — Spend extra time on topics that resonate with you\n4. **Run the code** — Every notebook contains executable examples; experiment with them\n5. **Read the papers** — We link to foundational literature throughout\n6. **Start your research project** — Apply what you've learned!\n\n**Repository structure:**\n\n```\ntopics/\n├── 01_getting started/\n│   ├── 01_overview/        ← You are here\n│   ├── 02_environment_setup/\n│   ├── 03_python_numpy_basics/\n│   ├── 04_opencv_fundamentals/\n│   └── 05_git_fundamentals/\n├── 02_classification/\n├── 03_localization/\n├── ...\n├── 15_embeddings_similarity/\n├── 16_vision_transformers/\n└── 17_self_supervised_learning/\n```\n\nEach topic folder contains:\n- `README.md` — Overview, prerequisites, and suggested path\n- `tutorial.ipynb` — Hands-on exercises\n- `project_template.ipynb` — Template for your own project",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "ae6c3ead",
   "metadata": {},
   "source": "## Areas of Study\n\nThis repository is organized into the following topics. Each section builds on foundational concepts while introducing specialized techniques.\n\n---\n\n### [02 — Image Classification](../../02_classification/)\nAssign a single label to an entire image.  \n*The conceptual starting point for most of modern computer vision.*\n\n---\n\n### [03 — Localization](../../03_localization/)\nPredict *where* an object is by drawing a bounding box.  \n*Adds spatial reasoning on top of classification.*\n\n---\n\n### [04 — Object Detection](../../04_object_detection/)\nDetect and localize multiple objects in a single image.  \n*The backbone of many real-world vision systems.*\n\n---\n\n### [05 — Segmentation](../../05_segmentation/)\nAssign a label to every pixel in the image.  \n*Provides precise object boundaries instead of coarse boxes.*\n\n---\n\n### [06 — Pose Estimation](../../06_pose_estimation/)\nInfer body or object configuration using keypoints.  \n*Critical for motion analysis and human–computer interaction.*\n\n---\n\n### [07 — Action Recognition](../../07_action_recognition/)\nRecognize what is happening over time in video.  \n*Introduces temporal reasoning into vision models.*\n\n---\n\n### [08 — Generative Models](../../08_generative_models/)\nLearn the data distribution well enough to generate new images.  \n*Forces models to understand structure, not just labels.*\n\n---\n\n### [09 — Visual SLAM](../../09_visual_slam/)\nSimultaneously localize a camera and build a map of the world.  \n*A geometry-heavy area central to robotics and AR.*\n\n---\n\n### [10 — Scene Understanding](../../10_scene_understanding/)\nReason about objects, layout, and relationships in a scene.  \n*Moves vision from perception toward meaning.*\n\n---\n\n### [11 — Facial Recognition](../../11_facial_recognition/)\nIdentify or verify individuals using facial features.  \n*Technically powerful and ethically sensitive.*\n\n---\n\n### [12 — Geolocation](../../12_geolocation/)\nPredict where an image was taken using visual cues alone.  \n*Combines vision, retrieval, and large-scale data.*\n\n---\n\n### [13 — Anomaly Detection](../../13_anomaly_detection/)\nDetect deviations from normal visual patterns.  \n*Often trained without ever seeing anomalies.*\n\n---\n\n### [14 — Vision-Based Reinforcement Learning](../../14_vision_rl/)\nLearn policies from visual input by interacting with an environment.  \n*Closes the loop between perception and action.*\n\n---\n\n### [15 — Embeddings & Similarity Search](../../15_embeddings_similarity/)\nMap images to vector representations for retrieval and matching.  \n*Essential for search, deduplication, and metric learning.*\n\n---\n\n### [16 — Vision Transformers](../../16_vision_transformers/)\nApply transformer architectures to visual data.  \n*Modern foundation models like ViT, CLIP, and SAM.*\n\n---\n\n### [17 — Self-Supervised Learning](../../17_self_supervised_learning/)\nLearn visual representations without manual labels.  \n*Enables training on massive unlabeled datasets.*"
  },
  {
   "cell_type": "markdown",
   "id": "a6b860a6",
   "metadata": {},
   "source": "---\n\n## References\n\n1. IBM. (2024). *What is Computer Vision?* Retrieved from [ibm.com/think/topics/computer-vision](https://www.ibm.com/think/topics/computer-vision)\n\n2. OpenAI. (2024). *ChatGPT.* [chatgpt.com](https://chatgpt.com)\n\n3. Anthropic. (2025). *Claude Code.* [claude.ai/claude-code](https://claude.ai/claude-code)"
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}